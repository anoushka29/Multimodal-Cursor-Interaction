# Multimodal-Cursor-Interaction

A cutting-edge AI-powered system transforming computer interaction through hand gesture recognition, eye gaze tracking, and voice command integration. Utilizing advanced tools like OpenCV, MediaPipe, Dlib, and Natural Language Processing (NLP), this project provides a cost-effective and intuitive alternative to traditional input devices. With 95% accuracy in gesture recognition and real-time responsiveness within 100ms, the system ensures precision, accessibility, and ease of use. Tested by over 100 users, it received 80% positive feedback and is specifically designed to assist individuals with mobility impairments. It also offers diverse applications in accessibility, gaming, and human-computer interaction research, making it ideal for exploring multimodal interaction and innovative interface designs.
